############################################### Generality ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Generality/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Generality/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Generality/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Generality/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Generality/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Generality/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Generality ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset databricks-dolly-15k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Generality/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3



############################################### Law ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Law/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Law/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Law/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Law/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Law/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Law/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Law ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset us_terms,Lawyer-Instruct --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Law/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3



############################################### Medicine ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Medicine/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Medicine/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Medicine/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Medicine/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Medicine/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Medicine/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Medicine ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset iCliniq,GenMedGPT-5k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Medicine/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3



############################################### Math ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Math/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Math/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Math/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Math/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Math/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Math/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Math ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset gsm8k --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Math/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3



############################################### Finance ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Finance/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Finance/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Finance/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Finance/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Finance/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Finance/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Finance ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset fineval-en --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Finance/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3



############################################### Mixed_Domains ###############################################

echo "********************************* Train Qwen2.5-7B/lora8 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/lora8 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora16 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/lora16 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora24 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/lora24 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 24 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/lora32 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/lora32 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 32 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/hydralora Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type hydralora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/hydralora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --lora_num 3

echo "********************************* Train Qwen2.5-7B/pissa Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/pissa --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --pissa_init

echo "********************************* Train Qwen2.5-7B/dora Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/dora --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --lora_rank 8 --max_samples 1000 --use_dora

echo "********************************* Train Qwen2.5-7B/prompt_tuning Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type prompt_tuning --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/prompt_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/ia3 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type ia3 --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/ia3 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/p_tuning Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type p_tuning --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/p_tuning --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000

echo "********************************* Train Qwen2.5-7B/cola1 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/cola1 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 1 --num_B 3

echo "********************************* Train Qwen2.5-7B/cola2 Mixed_Domains ********************************************"
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train --stage sft --do_train --model_name_or_path ./models/Qwen2.5-7B --dataset stem_zh --dataset_dir ./data --template qwen --finetuning_type cola --output_dir ./saves/Qwen2.5-7B/Mixed_Domains/cola2 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 50 --warmup_steps 20 --save_steps 100 --eval_steps 50 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 5.0  --val_size 0.1 --plot_loss --fp16 --max_samples 1000 --num_A 2 --num_B 3